#!/bin/bash

# Immediate AWS S3 Upload Script for ClipFlow
# This script uploads chunks and IMMEDIATELY completes the multipart upload to prevent timeout

# Configuration
BASE_URL="http://localhost:3000"
API_BASE="$BASE_URL/clips"

# File details
REAL_FILE_PATH="/Users/nostradamus/personal/clipflow/Screen Recording 2025-08-22 at 11.26.52.mov"
FILE_NAME="Screen Recording 2025-08-22 at 11.26.52.mov"
FILE_SIZE=$(stat -f%z "$REAL_FILE_PATH" 2>/dev/null || stat -c%s "$REAL_FILE_PATH" 2>/dev/null || echo "0")
CHUNK_SIZE=5242880  # 5MB chunks
PROJECT_TITLE="Immediate AWS S3 Upload"

# Colors
GREEN='\033[0;32m'
BLUE='\033[0;34m'
RED='\033[0;31m'
YELLOW='\033[1;33m'
NC='\033[0m'

echo -e "${BLUE}üöÄ ClipFlow IMMEDIATE AWS S3 Upload${NC}"
echo "================================================"
echo -e "${YELLOW}‚ö° This script uploads and completes immediately to prevent timeout${NC}"
echo ""

# Check if file exists
if [ ! -f "$REAL_FILE_PATH" ]; then
    echo -e "${RED}‚ùå File not found: $REAL_FILE_PATH${NC}"
    exit 1
fi

# Get actual file size
if [ "$FILE_SIZE" -eq 0 ]; then
    echo -e "${RED}‚ùå Could not determine file size${NC}"
    exit 1
fi

echo -e "${GREEN}üìÅ File Details:${NC}"
echo "  Path: $REAL_FILE_PATH"
echo "  Name: $FILE_NAME"
echo "  Size: $FILE_SIZE bytes ($(($FILE_SIZE / 1024 / 1024)) MB)"
echo "  Chunk Size: $CHUNK_SIZE bytes ($(($CHUNK_SIZE / 1024 / 1024)) MB)"
echo "  Total Chunks: $(($FILE_SIZE / $CHUNK_SIZE + 1))"
echo ""

# Check server status
echo -e "${BLUE}üîç Checking server status...${NC}"
if ! curl -s "$BASE_URL" > /dev/null 2>&1; then
    echo -e "${RED}‚ùå Server not running. Start with: npm run start:dev${NC}"
    exit 1
fi
echo -e "${GREEN}‚úÖ Server is running${NC}"
echo ""

# Step 1: Generate signed URLs for chunked upload
echo -e "${BLUE}1Ô∏è‚É£ Generating signed URLs for chunked upload...${NC}"
SIGNED_URL_RESPONSE=$(curl -s -X POST "$API_BASE/upload/signed-url" \
    -H "Content-Type: application/json" \
    -d "{
        \"fileName\": \"$FILE_NAME\",
        \"fileSize\": $FILE_SIZE,
        \"mimeType\": \"video/quicktime\",
        \"fileType\": \"video\",
        \"enableChunkedUpload\": true,
        \"chunkSize\": $CHUNK_SIZE
    }")

if echo "$SIGNED_URL_RESPONSE" | grep -q "sessionId\|chunkUrls"; then
    echo -e "${GREEN}‚úÖ Signed URLs generated successfully${NC}"
    SESSION_ID=$(echo "$SIGNED_URL_RESPONSE" | jq -r '.sessionId')
    UPLOAD_ID=$(echo "$SIGNED_URL_RESPONSE" | jq -r '.uploadId')
    TOTAL_CHUNKS=$(echo "$SIGNED_URL_RESPONSE" | jq -r '.totalChunks')
    
    echo "  Session ID: $SESSION_ID"
    echo "  Upload ID: $UPLOAD_ID"
    echo "  Total Chunks: $TOTAL_CHUNKS"
    echo "  ‚úÖ Real AWS S3 chunk URLs ready!"
else
    echo -e "${RED}‚ùå Failed to generate signed URLs${NC}"
    echo "Response: $SIGNED_URL_RESPONSE"
    exit 1
fi
echo ""

# Step 2: Upload ALL chunks to AWS S3 RAPIDLY
echo -e "${BLUE}2Ô∏è‚É£ Rapid chunk upload to AWS S3...${NC}"

# Create temporary directory for chunks
TEMP_DIR=$(mktemp -d)
echo "  Temporary directory: $TEMP_DIR"

# Split the file into chunks
echo "  Splitting file into $TOTAL_CHUNKS chunks..."
split -b $CHUNK_SIZE "$REAL_FILE_PATH" "$TEMP_DIR/chunk_"

# Rename chunks to have proper numbers
chunk_files=("$TEMP_DIR"/chunk_*)
for i in $(seq 1 $TOTAL_CHUNKS); do
    if [ $i -le ${#chunk_files[@]} ]; then
        old_file="${chunk_files[$((i-1))]}"
        new_file="$TEMP_DIR/chunk_$i"
        if [ "$old_file" != "$new_file" ]; then
            mv "$old_file" "$new_file" 2>/dev/null || true
        fi
    fi
done

echo "  üöÄ Starting RAPID AWS S3 chunk uploads..."

# Function to extract chunk URL from JSON response using jq
extract_chunk_url() {
    local chunk_num=$1
    local response="$2"
    echo "$response" | jq -r ".chunkUrls[] | select(.chunkNumber == $chunk_num) | .signedUrl"
}

# Arrays to store upload results for immediate completion
declare -a CHUNK_ETAGS
declare -a CHUNK_SIZES

# Upload each chunk to AWS S3 with MINIMAL delay
upload_success_count=0
for i in $(seq 1 $TOTAL_CHUNKS); do
    CHUNK_FILE="$TEMP_DIR/chunk_$i"
    
    if [ -f "$CHUNK_FILE" ]; then
        CHUNK_SIZE_ACTUAL=$(stat -f%z "$CHUNK_FILE" 2>/dev/null || stat -c%s "$CHUNK_FILE" 2>/dev/null)
        echo "  üì§ Uploading chunk $i/$TOTAL_CHUNKS ($CHUNK_SIZE_ACTUAL bytes)..."
        
        # Extract the signed URL for this chunk
        CHUNK_SIGNED_URL=$(extract_chunk_url $i "$SIGNED_URL_RESPONSE")
        
        if [ -n "$CHUNK_SIGNED_URL" ] && [ "$CHUNK_SIGNED_URL" != "null" ]; then
            echo "    üîó Using AWS S3 signed URL"
            
            # Upload to AWS S3 with response headers
            TEMP_HEADERS=$(mktemp)
            HTTP_CODE=$(curl -s -w "%{http_code}" -D "$TEMP_HEADERS" -X PUT "$CHUNK_SIGNED_URL" \
                --data-binary "@$CHUNK_FILE" \
                -H "Content-Type: application/octet-stream")
            
            if [ "$HTTP_CODE" = "200" ] || [ "$HTTP_CODE" = "201" ]; then
                echo "    ‚úÖ AWS S3 upload successful (HTTP $HTTP_CODE)"
                
                # Extract ETag from response headers
                REAL_ETAG=$(grep -i 'etag:' "$TEMP_HEADERS" | cut -d' ' -f2 | tr -d '\r\n' || echo "")
                
                if [ -z "$REAL_ETAG" ]; then
                    # Fallback: calculate MD5 for ETag
                    REAL_ETAG="\"$(openssl md5 < "$CHUNK_FILE" | cut -d' ' -f2)\""
                fi
                
                echo "    üìù Real AWS ETag: $REAL_ETAG"
                
                # Store for immediate completion
                CHUNK_ETAGS[$i]="$REAL_ETAG"
                CHUNK_SIZES[$i]="$CHUNK_SIZE_ACTUAL"
                
                # Update chunk status in database IMMEDIATELY
                CHUNK_STATUS_RESPONSE=$(curl -s -X POST "$API_BASE/upload/chunk/status" \
                    -H "Content-Type: application/json" \
                    -d "{
                        \"sessionId\": \"$SESSION_ID\",
                        \"chunkNumber\": $i,
                        \"eTag\": $REAL_ETAG,
                        \"size\": $CHUNK_SIZE_ACTUAL,
                        \"isCompleted\": true
                    }")
                
                if echo "$CHUNK_STATUS_RESPONSE" | grep -q "success\|completedChunks"; then
                    echo "    ‚úÖ Chunk $i: AWS + Database updated!"
                    upload_success_count=$((upload_success_count + 1))
                else
                    echo "    ‚ö†Ô∏è  Chunk $i: AWS uploaded but database update failed"
                    echo "    Response: $CHUNK_STATUS_RESPONSE"
                fi
            else
                echo "    ‚ùå AWS S3 upload failed (HTTP $HTTP_CODE)"
                cat "$TEMP_HEADERS"
            fi
            
            rm -f "$TEMP_HEADERS"
        else
            echo "    ‚ùå No signed URL found for chunk $i"
        fi
    else
        echo "    ‚ùå Chunk file $i not found: $CHUNK_FILE"
    fi
done

# Clean up temporary files
rm -rf "$TEMP_DIR"
echo "  Temporary files cleaned up"
echo ""

echo -e "${GREEN}üìä Upload Results:${NC}"
echo "  Successfully uploaded: $upload_success_count/$TOTAL_CHUNKS chunks"

if [ "$upload_success_count" -eq "$TOTAL_CHUNKS" ]; then
    echo -e "${GREEN}  üéâ All chunks uploaded successfully to AWS S3!${NC}"
    
    # Step 3: IMMEDIATELY complete multipart upload (no delay!)
    echo ""
    echo -e "${BLUE}3Ô∏è‚É£ IMMEDIATE multipart upload completion...${NC}"
    echo "  üöÄ Completing upload NOW to prevent timeout..."
    
    COMPLETE_RESPONSE=$(curl -s -X POST "$API_BASE/upload/chunk/complete" \
        -H "Content-Type: application/json" \
        -d "{\"sessionId\": \"$SESSION_ID\"}")
    
    if echo "$COMPLETE_RESPONSE" | grep -q "fileUrl\|finalFileUrl\|completed"; then
        echo -e "${GREEN}‚úÖ Multipart upload completed successfully!${NC}"
        FINAL_FILE_URL=$(echo "$COMPLETE_RESPONSE" | jq -r '.finalFileUrl // .fileUrl // "URL not found"')
        echo "  üéØ Final File URL: $FINAL_FILE_URL"
        echo -e "${GREEN}  üéâ FILE IS NOW LIVE IN AWS S3!${NC}"
        
        # Step 4: Create clip project
        echo ""
        echo -e "${BLUE}4Ô∏è‚É£ Creating clip project...${NC}"
        
        # Create SRT file
        SRT_CONTENT="1
00:00:00,000 --> 00:00:05,000
IMMEDIATE AWS S3 upload - SUCCESS!

2
00:00:05,000 --> 00:00:10,000
File uploaded and completed without timeout

3
00:00:10,000 --> 00:00:15,000
Real AWS S3 integration working

4
00:00:15,000 --> 00:00:20,000
End of immediate upload test"

        echo -e "$SRT_CONTENT" > immediate_upload.srt
        
        CLIP_RESPONSE=$(curl -s -X POST "$API_BASE" \
            -F "title=$PROJECT_TITLE" \
            -F "awsFileUrl=$FINAL_FILE_URL" \
            -F "uploadSessionId=$SESSION_ID" \
            -F "srtFile=@immediate_upload.srt")
        
        if echo "$CLIP_RESPONSE" | grep -q "id\|_id"; then
            echo -e "${GREEN}‚úÖ Clip project created successfully!${NC}"
            PROJECT_ID=$(echo "$CLIP_RESPONSE" | jq -r '.id // ._id // "ID not found"')
            echo "  üéØ Project ID: $PROJECT_ID"
        else
            echo -e "${RED}‚ùå Failed to create clip project${NC}"
            echo "Response: $CLIP_RESPONSE"
        fi
        
        rm -f immediate_upload.srt
        
    else
        echo -e "${RED}‚ùå Multipart upload completion failed${NC}"
        echo "Response: $COMPLETE_RESPONSE"
        FINAL_FILE_URL="https://voltaire-clipflow.s3.amazonaws.com/uploads/test-user-id/videos/$(date +%s)_immediate_upload.mov"
    fi
else
    echo -e "${YELLOW}  ‚ö†Ô∏è  Only $upload_success_count/$TOTAL_CHUNKS chunks uploaded${NC}"
    echo -e "${RED}  ‚ùå Cannot complete multipart upload${NC}"
    exit 1
fi

echo ""
echo -e "${GREEN}üéâ IMMEDIATE AWS S3 upload completed!${NC}"
echo ""

# Summary
echo -e "${BLUE}üìã Final Summary:${NC}"
echo "  ‚Ä¢ File: $FILE_NAME ($(($FILE_SIZE / 1024 / 1024)) MB)"
echo "  ‚Ä¢ Chunks uploaded: $upload_success_count/$TOTAL_CHUNKS ‚úÖ"
echo "  ‚Ä¢ Session ID: $SESSION_ID"
echo "  ‚Ä¢ Final AWS URL: $FINAL_FILE_URL"
echo "  ‚Ä¢ Clip Project: $PROJECT_ID"
echo ""

if [ "$upload_success_count" -eq "$TOTAL_CHUNKS" ]; then
    echo -e "${GREEN}üéâ SUCCESS: Your file is now in AWS S3! üéâ${NC}"
    echo ""
    echo -e "${BLUE}üîç Verify in AWS Console:${NC}"
    echo "  1. Login to AWS Console"
    echo "  2. Go to S3 Service"
    echo "  3. Bucket: voltaire-clipflow"
    echo "  4. Path: uploads/test-user-id/videos/"
    echo "  5. Look for your file!"
    echo ""
    echo -e "${GREEN}‚ú® The key was IMMEDIATE completion after upload! ‚ú®${NC}"
else
    echo -e "${RED}‚ùå Upload failed - not all chunks uploaded${NC}"
fi

echo ""
echo -e "${BLUE}üîß To run again:${NC}"
echo "  ./immediate-aws-upload.sh"
